
#https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbmTuning.Rmd## H2O is an R package
library(h2o)
## Create an H2O cloud 
h2o.init(
  nthreads=-1,            ## -1: use all available threads
  max_mem_size = "2G")    ## specify the memory size for the H2O cloud
h2o.removeAll() # Clean slate - just in case the cluster was already running

#set location
setwd("c:/Users/Eli/Desktop/Data Orbital Testing Files/Project4") 

#read data
df <- h2o.importFile(path = normalizePath("CleanedData.csv"))
newvalid<- h2o.importFile(path = normalizePath("newvalid.csv"))
df$Party <- as.factor(df$Party)
df$Age <- as.factor(df$Age)
df$CD <- as.factor(df$CD)
df$Gender <- as.factor(df$Gender)
df$Race <- as.factor(df$Race)
df$y <- as.factor(df$y)
summary(df,exact_quantiles=TRUE)
response <- "y"
## use all other columns (except for the name) as predictors
predictors <- setdiff(names(df), c(response, "UID","total")) 

splits <- h2o.splitFrame(
  df,     
  c(0.9,0.05), 
  destination_frames = c("train.hex", "valid.hex", "test.hex"),
  seed=1234) 

train <- splits[[1]]
valid <- splits[[2]]
test  <- splits[[3]]

## take a look at the first few rows of the data set
train[1:5,] 



#----------------------------------------------------
#baseline model
## We only provide the required parameters, everything else is default
gbm <- h2o.gbm(x = predictors, y = response, training_frame = train)

## Show a detailed model summary
gbm

## Get the AUC on the validation set
h2o.auc(h2o.performance(gbm, newdata = valid)) 
#------------------------------------------------------
#second modelï¼Œusing 0.8 data for training
## h2o.rbind makes a copy here, so it's better to use splitFrame with `ratios = c(0.8)` instead above
gbm <- h2o.gbm(x = predictors, y = response, training_frame = h2o.rbind(train, valid), nfolds = 4, seed = 0xDECAF)

## Show a detailed summary of the cross validation metrics
## This gives you an idea of the variance between the folds
gbm@model$cross_validation_metrics_summary

## Get the cross-validated AUC by scoring the combined holdout predictions.
## (Instead of taking the average of the metrics across the folds)
h2o.auc(h2o.performance(gbm, xval = TRUE))
#------------------------------------
#third model is the best model
gbm <- h2o.gbm(
  ## standard model parameters
  x = predictors, 
  y = response, 
  training_frame = train, 
  validation_frame = valid,
  
  ## more trees is better if the learning rate is small enough 
  ## here, use "more than enough" trees - we have early stopping
  ntrees = 10000,                                                            
  
  ## smaller learning rate is better (this is a good value for most datasets, but see below for annealing)
  learn_rate=0.01,                                                         
  
  ## early stopping once the validation AUC doesn't improve by at least 0.01% for 5 consecutive scoring events
  stopping_rounds = 5, stopping_tolerance = 1e-4, stopping_metric = "AUC", 
  
  ## sample 80% of rows per tree
  sample_rate = 0.8,                                                       
  
  ## sample 80% of columns per split
  col_sample_rate = 0.8,                                                   
  
  ## fix a random number generator seed for reproducibility
  seed = 1234,                                                             
  
  ## score every 10 trees to make early stopping reproducible (it depends on the scoring interval)
  score_tree_interval = 10                                                 
)
summary(gbm)
## Get the AUC on the validation set
h2o.auc(h2o.performance(gbm, valid = TRUE))
#------------------------------------------------------------
preds <- h2o.predict(gbm, newvalid)
head(preds)
h2o.exportFile(preds, "C:/Users/Eli/Desktop/Data Orbital Testing Files/Project4/H2OScored.csv", force=TRUE)


gbm@model$validation_metrics@metrics$max_criteria_and_metric_scores
head(preds)
gbm@model$validation_metrics@metrics$max_criteria_and_metric_scores
#-----------------------------------------------------------------------
